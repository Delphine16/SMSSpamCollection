{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive MLP to detect spam\n",
    "Here: build the most basic solution that classifies emails.\n",
    "We don't expect a good solution, just something that works so that we can implement changes later on, one at a time.\n",
    "\n",
    "Naive ideas :\n",
    "Word embedder : naive solution which consists of going through all the words in the file and assigning them the order in which they are encountered.\n",
    "Classifier itself : MLP\n",
    "\n",
    "It is also necessary to clean the data (transform to lowercase ...) but I describe that in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd # conda install -c anaconda pandas\n",
    "\n",
    "#Deep learning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Text processing\n",
    "import tensorflow as tf # conda install -c conda-forge tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "A tokenizer converts a string, such as a sentence, into individual tokens. These may be words or numbers. The simplest tokenizer consists in separating a sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here’s',\n",
       " 'to',\n",
       " 'the',\n",
       " 'crazy',\n",
       " 'ones,',\n",
       " 'the',\n",
       " 'misfits,',\n",
       " 'the',\n",
       " 'rebels,',\n",
       " 'the',\n",
       " 'troublemakers']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Here’s to the crazy ones, the misfits, the rebels, the troublemakers\"\n",
    "text.split() # python method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several details to take into account. For example, whether to include punctuation and how to handle upper case letters. The tokenizer method from tensorflow.keras allows us to transform text into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5, 1, 6, 1, 7, 1, 8]]\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow.keras method\n",
    "text2 = [\"Here’s to the crazy ones, the misfits, the rebels, the troublemakers\"]\n",
    "        # LIST containing our string, not the same thing I don't know why\n",
    "tokenizer = Tokenizer(num_words=50, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(text2) \n",
    "sequences = tokenizer.texts_to_sequences(text2)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine each command.\n",
    "\n",
    "*fit_on_texts* updates the vocabulary of our tokenizer. Each word is assigned an index based on how frequent it appears in our text. So the most common word gets assigned 1, the second most common 2 and so on. The index 0 is reserved for padding.\n",
    "\n",
    "We can define a limit to the size of our vocabulary, defined in the first line with *num_word=50*, which can allow us to filter out rare words. The tokenizer method filters out all punctuation by default, and *lower=True* converts all uppercase characters to lowercase. Setting *char_level=False* simply means we split at the level of words rather than at the level of characters.\n",
    "\n",
    "*texts_to_sequences* transforms the given text into a sequence of indices, using the internal vocabulary of our tokenizer. Having the two commands be separate allows us to \"train\" our vocabulary on one text and convert any new text we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'here’s': 2,\n",
       " 'to': 3,\n",
       " 'crazy': 4,\n",
       " 'ones': 5,\n",
       " 'misfits': 6,\n",
       " 'rebels': 7,\n",
       " 'troublemakers': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "Following the tutorial on https://towardsdatascience.com/nlp-spam-detection-in-sms-text-data-using-deep-learning-b8632db85cc8 to get something started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ThinhNguyendai/SMSSpamDetection/main/SMSSpamCollection\" #Use the RAW one\n",
    "messages = pd.read_csv(url, sep ='\\t', names=[\"label\", \"message\"])\n",
    "messages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label                                            message\n",
      "103   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "154   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "207   ham  As I entered my cabin my PA said, '' Happy B'd...\n",
      "223   ham                             Sorry, I'll call later\n",
      "326   ham                   No calls..messages..missed calls\n",
      "339   ham                             Sorry, I'll call later\n",
      "357  spam  Congratulations ur awarded 500 of CD vouchers ...\n",
      "444   ham                             Sorry, I'll call later\n",
      "533   ham                  Gudnite....tc...practice going on\n",
      "655   ham                       Did u got that persons story\n"
     ]
    }
   ],
   "source": [
    "duplicatedRow = messages[messages.duplicated()]\n",
    "print(duplicatedRow[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5572 messages, 4825 of which are ham and 747 are spam. The dataset is **highly unbalanced, so we need to do something about it.**. We also have 403 duplicate messages.\n",
    "\n",
    "There are many ways to handle unbalanced dataset, and it is worth exploring other ways than the one shown in the link. The author uses downsampling, which simply deletes observations from the class that is overrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_msg = messages[messages.label =='ham']\n",
    "spam_msg = messages[messages.label=='spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747, 2) (747, 2)\n"
     ]
    }
   ],
   "source": [
    "ham_msg_df = ham_msg.sample(n = len(spam_msg), random_state = 704)\n",
    "spam_msg_df = spam_msg\n",
    "print(ham_msg_df.shape, spam_msg_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Maybe?! Say hi to  and find out if  got his ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Jos ask if u wana meet up?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>I dont know oh. Hopefully this month.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dear Hero,i am leaving to qatar tonite for an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>No that just means you have a fat head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>spam</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>spam</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Maybe?! Say hi to  and find out if  got his ca...\n",
       "1      ham                         Jos ask if u wana meet up?\n",
       "2      ham              I dont know oh. Hopefully this month.\n",
       "3      ham  Dear Hero,i am leaving to qatar tonite for an ...\n",
       "4      ham             No that just means you have a fat head\n",
       "...    ...                                                ...\n",
       "1489  spam  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "1490  spam  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "1491  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "1492  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "1493  spam  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_df = pd.concat([ham_msg_df, spam_msg_df])\n",
    "msg_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_labels = (msg_df['label'].map({'ham': 0, 'spam': 1})).values\n",
    "            # map creates a dataframe where we replace values\n",
    "            # .values is to extract the values as an array\n",
    "msg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_msg, test_msg, train_labels, test_labels = train_test_split(msg_df['message'],\n",
    "                                                                  msg_labels,\n",
    "                                                                  test_size=0.2,\n",
    "                                                                  random_state=705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5039    Thanks for being there for me just to talk to ...\n",
      "1122    Do you want 750 anytime any network mins 150 t...\n",
      "1751                           Got it..mail panren paru..\n",
      "1376                              We're finally ready fyi\n",
      "3906    Do you want a new video handset? 750 anytime a...\n",
      "                              ...                        \n",
      "3562    Text BANNEDUK to 89555 to see! cost 150p texto...\n",
      "5537    Want explicit SEX in 30 secs? Ring 02073162414...\n",
      "1952    Haha... Really oh no... How? Then will they de...\n",
      "4432       2mro i am not coming to gym machan. Goodnight.\n",
      "2808    December only! Had your mobile 11mths+? You ar...\n",
      "Name: message, Length: 1195, dtype: object\n",
      "\n",
      "\n",
      "[0 1 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_msg)\n",
    "print(\"\\n\")\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer : turn words into integers\n",
    "oov_tok = \"<OOV>\" # What to replace words that are not in the vocabulary with\n",
    "vocab_size = 500 # Maximum number of words for tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size,\n",
    "                      char_level=False, # Work words by word\n",
    "                      oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'to': 2,\n",
       " 'you': 3,\n",
       " 'a': 4,\n",
       " 'i': 5,\n",
       " 'call': 6,\n",
       " 'the': 7,\n",
       " 'your': 8,\n",
       " 'u': 9,\n",
       " 'for': 10,\n",
       " 'is': 11,\n",
       " 'now': 12,\n",
       " 'and': 13,\n",
       " '2': 14,\n",
       " 'free': 15,\n",
       " 'or': 16,\n",
       " 'on': 17,\n",
       " 'in': 18,\n",
       " 'have': 19,\n",
       " 'ur': 20,\n",
       " 'txt': 21,\n",
       " 'me': 22,\n",
       " 'from': 23,\n",
       " '4': 24,\n",
       " 'of': 25,\n",
       " 'are': 26,\n",
       " 'text': 27,\n",
       " 'with': 28,\n",
       " 'it': 29,\n",
       " 'this': 30,\n",
       " 'get': 31,\n",
       " 'mobile': 32,\n",
       " 'just': 33,\n",
       " 'stop': 34,\n",
       " 'no': 35,\n",
       " 'my': 36,\n",
       " 'only': 37,\n",
       " 'reply': 38,\n",
       " 'claim': 39,\n",
       " 'will': 40,\n",
       " 'send': 41,\n",
       " 'out': 42,\n",
       " 'so': 43,\n",
       " 'if': 44,\n",
       " 'that': 45,\n",
       " 'be': 46,\n",
       " 'we': 47,\n",
       " 'our': 48,\n",
       " 'prize': 49,\n",
       " 'www': 50,\n",
       " 'can': 51,\n",
       " 'do': 52,\n",
       " 'not': 53,\n",
       " 'at': 54,\n",
       " 'cash': 55,\n",
       " 'but': 56,\n",
       " 'won': 57,\n",
       " '150p': 58,\n",
       " 'new': 59,\n",
       " 't': 60,\n",
       " 'win': 61,\n",
       " 'msg': 62,\n",
       " '1': 63,\n",
       " 'please': 64,\n",
       " 'phone': 65,\n",
       " '50': 66,\n",
       " 'who': 67,\n",
       " 'uk': 68,\n",
       " 'week': 69,\n",
       " 'urgent': 70,\n",
       " 'go': 71,\n",
       " 'all': 72,\n",
       " 'nokia': 73,\n",
       " 'tone': 74,\n",
       " \"i'm\": 75,\n",
       " 'service': 76,\n",
       " 'when': 77,\n",
       " 'min': 78,\n",
       " 'what': 79,\n",
       " 'know': 80,\n",
       " 'want': 81,\n",
       " 'r': 82,\n",
       " 'by': 83,\n",
       " 'c': 84,\n",
       " 'up': 85,\n",
       " 'been': 86,\n",
       " 'good': 87,\n",
       " 'back': 88,\n",
       " 'contact': 89,\n",
       " 'how': 90,\n",
       " '16': 91,\n",
       " '18': 92,\n",
       " 'customer': 93,\n",
       " 'co': 94,\n",
       " 'com': 95,\n",
       " 'message': 96,\n",
       " 'per': 97,\n",
       " '£1': 98,\n",
       " 'lt': 99,\n",
       " 'gt': 100,\n",
       " 'then': 101,\n",
       " 'time': 102,\n",
       " 'guaranteed': 103,\n",
       " 'chat': 104,\n",
       " 'day': 105,\n",
       " 'its': 106,\n",
       " 'as': 107,\n",
       " 'today': 108,\n",
       " 'got': 109,\n",
       " 'see': 110,\n",
       " 'ok': 111,\n",
       " 'hi': 112,\n",
       " 'find': 113,\n",
       " '3': 114,\n",
       " 'n': 115,\n",
       " 'he': 116,\n",
       " 'every': 117,\n",
       " '£1000': 118,\n",
       " 'cs': 119,\n",
       " 'was': 120,\n",
       " 'am': 121,\n",
       " 'more': 122,\n",
       " 'love': 123,\n",
       " 'like': 124,\n",
       " 'any': 125,\n",
       " 'mins': 126,\n",
       " 'ringtone': 127,\n",
       " 'holiday': 128,\n",
       " 'has': 129,\n",
       " 'an': 130,\n",
       " 'come': 131,\n",
       " 'camera': 132,\n",
       " 'there': 133,\n",
       " 'receive': 134,\n",
       " 'draw': 135,\n",
       " 'k': 136,\n",
       " 'yes': 137,\n",
       " 'sms': 138,\n",
       " 'line': 139,\n",
       " 'live': 140,\n",
       " 'right': 141,\n",
       " 'going': 142,\n",
       " 'awarded': 143,\n",
       " 'latest': 144,\n",
       " 'one': 145,\n",
       " 'video': 146,\n",
       " 'dont': 147,\n",
       " 'they': 148,\n",
       " 'apply': 149,\n",
       " \"don't\": 150,\n",
       " 'wk': 151,\n",
       " 'about': 152,\n",
       " 'number': 153,\n",
       " 'd': 154,\n",
       " 'ü': 155,\n",
       " 'tomorrow': 156,\n",
       " 'rate': 157,\n",
       " 'tell': 158,\n",
       " 'landline': 159,\n",
       " 'code': 160,\n",
       " 'take': 161,\n",
       " 'box': 162,\n",
       " 'night': 163,\n",
       " 'da': 164,\n",
       " 'network': 165,\n",
       " 'her': 166,\n",
       " '150ppm': 167,\n",
       " 'next': 168,\n",
       " 'pls': 169,\n",
       " 'award': 170,\n",
       " 'chance': 171,\n",
       " 'shows': 172,\n",
       " 'think': 173,\n",
       " 's': 174,\n",
       " 'offer': 175,\n",
       " '000': 176,\n",
       " 'word': 177,\n",
       " 'had': 178,\n",
       " 'orange': 179,\n",
       " 'make': 180,\n",
       " 'entry': 181,\n",
       " 'po': 182,\n",
       " 'need': 183,\n",
       " 'weekly': 184,\n",
       " 'tones': 185,\n",
       " 'over': 186,\n",
       " '£100': 187,\n",
       " 'special': 188,\n",
       " '1st': 189,\n",
       " 'selected': 190,\n",
       " '5': 191,\n",
       " \"i'll\": 192,\n",
       " 'some': 193,\n",
       " 'give': 194,\n",
       " 'collection': 195,\n",
       " '£5000': 196,\n",
       " 'here': 197,\n",
       " 'thanks': 198,\n",
       " 'collect': 199,\n",
       " 'name': 200,\n",
       " 'play': 201,\n",
       " 'she': 202,\n",
       " 'later': 203,\n",
       " 'work': 204,\n",
       " '10p': 205,\n",
       " 'attempt': 206,\n",
       " 'lor': 207,\n",
       " \"it's\": 208,\n",
       " 'update': 209,\n",
       " 'mob': 210,\n",
       " 'poly': 211,\n",
       " 'sae': 212,\n",
       " '500': 213,\n",
       " 'last': 214,\n",
       " 'sorry': 215,\n",
       " 'valid': 216,\n",
       " 'account': 217,\n",
       " 'help': 218,\n",
       " 'national': 219,\n",
       " 'dear': 220,\n",
       " 'delivery': 221,\n",
       " 'yours': 222,\n",
       " '8007': 223,\n",
       " 'bonus': 224,\n",
       " 'real': 225,\n",
       " 'him': 226,\n",
       " '10': 227,\n",
       " 'opt': 228,\n",
       " '2nd': 229,\n",
       " 'why': 230,\n",
       " 'e': 231,\n",
       " 'b': 232,\n",
       " 'music': 233,\n",
       " 'club': 234,\n",
       " \"c's\": 235,\n",
       " 'colour': 236,\n",
       " 'well': 237,\n",
       " 'home': 238,\n",
       " 'top': 239,\n",
       " 'great': 240,\n",
       " 'calls': 241,\n",
       " 'still': 242,\n",
       " 'join': 243,\n",
       " 'await': 244,\n",
       " 'http': 245,\n",
       " 'cant': 246,\n",
       " '£500': 247,\n",
       " 'meet': 248,\n",
       " 'very': 249,\n",
       " '750': 250,\n",
       " 'half': 251,\n",
       " 'v': 252,\n",
       " 'keep': 253,\n",
       " 'vouchers': 254,\n",
       " '86688': 255,\n",
       " 'hot': 256,\n",
       " 'after': 257,\n",
       " 'cost': 258,\n",
       " 'first': 259,\n",
       " 'oh': 260,\n",
       " 'dating': 261,\n",
       " 'texts': 262,\n",
       " 'games': 263,\n",
       " 'should': 264,\n",
       " 'wat': 265,\n",
       " 'pounds': 266,\n",
       " 'price': 267,\n",
       " 'miss': 268,\n",
       " 'quiz': 269,\n",
       " 'which': 270,\n",
       " 'us': 271,\n",
       " 'someone': 272,\n",
       " 'life': 273,\n",
       " 'yeah': 274,\n",
       " 'waiting': 275,\n",
       " 'g': 276,\n",
       " 'pick': 277,\n",
       " 'auction': 278,\n",
       " 'row': 279,\n",
       " 'wanna': 280,\n",
       " '6': 281,\n",
       " 'too': 282,\n",
       " 'best': 283,\n",
       " 'hey': 284,\n",
       " 'sent': 285,\n",
       " 'did': 286,\n",
       " 'trying': 287,\n",
       " 'were': 288,\n",
       " 'say': 289,\n",
       " 'pic': 290,\n",
       " '£3': 291,\n",
       " 'private': 292,\n",
       " 'todays': 293,\n",
       " 'x': 294,\n",
       " '£2000': 295,\n",
       " 'order': 296,\n",
       " 'xmas': 297,\n",
       " 'end': 298,\n",
       " 'hello': 299,\n",
       " 'messages': 300,\n",
       " 'ltd': 301,\n",
       " 'gift': 302,\n",
       " 'tried': 303,\n",
       " 'services': 304,\n",
       " 'bt': 305,\n",
       " 'days': 306,\n",
       " 'anytime': 307,\n",
       " '08000930705': 308,\n",
       " 'final': 309,\n",
       " 'winner': 310,\n",
       " 'operator': 311,\n",
       " '7': 312,\n",
       " 'xxx': 313,\n",
       " 'even': 314,\n",
       " 'sexy': 315,\n",
       " 'm': 316,\n",
       " 'expires': 317,\n",
       " 'land': 318,\n",
       " 'could': 319,\n",
       " 'hope': 320,\n",
       " 'savamob': 321,\n",
       " 'content': 322,\n",
       " '£2': 323,\n",
       " 'buy': 324,\n",
       " 'already': 325,\n",
       " 'suite342': 326,\n",
       " '2lands': 327,\n",
       " 'heart': 328,\n",
       " \"i've\": 329,\n",
       " 'try': 330,\n",
       " 're': 331,\n",
       " '03': 332,\n",
       " 'off': 333,\n",
       " 'freemsg': 334,\n",
       " 'being': 335,\n",
       " 'talk': 336,\n",
       " 'ready': 337,\n",
       " 'camcorder': 338,\n",
       " 'congrats': 339,\n",
       " 'looking': 340,\n",
       " 'reveal': 341,\n",
       " 'pobox': 342,\n",
       " 'enjoy': 343,\n",
       " '12hrs': 344,\n",
       " 'his': 345,\n",
       " 'statement': 346,\n",
       " 'points': 347,\n",
       " 'identifier': 348,\n",
       " 'amp': 349,\n",
       " 'net': 350,\n",
       " 'between': 351,\n",
       " 'tc': 352,\n",
       " 'happy': 353,\n",
       " 'dogging': 354,\n",
       " 'worth': 355,\n",
       " 'offers': 356,\n",
       " 'way': 357,\n",
       " '0800': 358,\n",
       " 'where': 359,\n",
       " 'f': 360,\n",
       " 'part': 361,\n",
       " 'weekend': 362,\n",
       " 'wait': 363,\n",
       " 'sleep': 364,\n",
       " 'eg': 365,\n",
       " 'use': 366,\n",
       " 'caller': 367,\n",
       " 'charged': 368,\n",
       " 'either': 369,\n",
       " 'leave': 370,\n",
       " 'unsubscribe': 371,\n",
       " 'many': 372,\n",
       " 'rental': 373,\n",
       " 'before': 374,\n",
       " 'wkly': 375,\n",
       " 'question': 376,\n",
       " 'std': 377,\n",
       " 'mates': 378,\n",
       " 'speak': 379,\n",
       " 'info': 380,\n",
       " '£250': 381,\n",
       " 'shopping': 382,\n",
       " 'friends': 383,\n",
       " 'babe': 384,\n",
       " 'details': 385,\n",
       " 'representative': 386,\n",
       " 'ever': 387,\n",
       " 'baby': 388,\n",
       " 'girl': 389,\n",
       " 'tv': 390,\n",
       " 'complimentary': 391,\n",
       " 'place': 392,\n",
       " 'welcome': 393,\n",
       " 'important': 394,\n",
       " 'hg': 395,\n",
       " 'voucher': 396,\n",
       " 'wap': 397,\n",
       " 'money': 398,\n",
       " 'age': 399,\n",
       " 'much': 400,\n",
       " '08000839402': 401,\n",
       " 'date': 402,\n",
       " 'ac': 403,\n",
       " '£350': 404,\n",
       " 'double': 405,\n",
       " 'fun': 406,\n",
       " 'having': 407,\n",
       " 'secret': 408,\n",
       " 'admirer': 409,\n",
       " 'thinks': 410,\n",
       " 'comp': 411,\n",
       " 'may': 412,\n",
       " 'flag': 413,\n",
       " 'news': 414,\n",
       " 'answer': 415,\n",
       " 'easy': 416,\n",
       " 'late': 417,\n",
       " 'player': 418,\n",
       " 'pics': 419,\n",
       " 'direct': 420,\n",
       " 'care': 421,\n",
       " 'month': 422,\n",
       " 'mobiles': 423,\n",
       " 'haha': 424,\n",
       " \"there's\": 425,\n",
       " 'cos': 426,\n",
       " '00': 427,\n",
       " 'sub': 428,\n",
       " 'okay': 429,\n",
       " 'del': 430,\n",
       " 'pm': 431,\n",
       " 'choose': 432,\n",
       " 'txts': 433,\n",
       " 'msgs': 434,\n",
       " 'wish': 435,\n",
       " 'phones': 436,\n",
       " 'other': 437,\n",
       " 'coming': 438,\n",
       " 'fancy': 439,\n",
       " 'sex': 440,\n",
       " 'them': 441,\n",
       " 'let': 442,\n",
       " 'ipod': 443,\n",
       " 'congratulations': 444,\n",
       " '11': 445,\n",
       " 'balance': 446,\n",
       " 'mobileupd8': 447,\n",
       " 'im': 448,\n",
       " '£200': 449,\n",
       " 'yo': 450,\n",
       " 'morning': 451,\n",
       " '100': 452,\n",
       " 'told': 453,\n",
       " 'o': 454,\n",
       " 'charity': 455,\n",
       " '150': 456,\n",
       " 'pass': 457,\n",
       " 'england': 458,\n",
       " 'inc': 459,\n",
       " 'custcare': 460,\n",
       " 'rates': 461,\n",
       " 'something': 462,\n",
       " 'minutes': 463,\n",
       " \"you're\": 464,\n",
       " 'found': 465,\n",
       " '2003': 466,\n",
       " '800': 467,\n",
       " '04': 468,\n",
       " 'man': 469,\n",
       " 'charge': 470,\n",
       " 'really': 471,\n",
       " '2004': 472,\n",
       " 'because': 473,\n",
       " 'kiss': 474,\n",
       " 'ask': 475,\n",
       " '£150': 476,\n",
       " 'discount': 477,\n",
       " 'valued': 478,\n",
       " 'guess': 479,\n",
       " 'done': 480,\n",
       " 'reward': 481,\n",
       " 'lucky': 482,\n",
       " '20p': 483,\n",
       " 'ntt': 484,\n",
       " 'tonight': 485,\n",
       " 'hear': 486,\n",
       " 'years': 487,\n",
       " 'gr8': 488,\n",
       " 'address': 489,\n",
       " 'getting': 490,\n",
       " 'start': 491,\n",
       " 'reach': 492,\n",
       " 'wid': 493,\n",
       " 'currently': 494,\n",
       " 'w1j6hl': 495,\n",
       " 'call2optout': 496,\n",
       " 'within': 497,\n",
       " 'nice': 498,\n",
       " '87066': 499,\n",
       " 'numbers': 500,\n",
       " 'eve': 501,\n",
       " 'sure': 502,\n",
       " 'friend': 503,\n",
       " 'polys': 504,\n",
       " 'away': 505,\n",
       " 'rcvd': 506,\n",
       " 'saturday': 507,\n",
       " 'etc': 508,\n",
       " 'bx420': 509,\n",
       " 'ip4': 510,\n",
       " '5we': 511,\n",
       " 'maybe': 512,\n",
       " 'problem': 513,\n",
       " 'txting': 514,\n",
       " 'game': 515,\n",
       " 'specially': 516,\n",
       " 'ill': 517,\n",
       " 'true': 518,\n",
       " 'hmv': 519,\n",
       " 'weeks': 520,\n",
       " 'anyway': 521,\n",
       " 'sweet': 522,\n",
       " 'sony': 523,\n",
       " 'dvd': 524,\n",
       " '82277': 525,\n",
       " 'sp': 526,\n",
       " '50p': 527,\n",
       " 'into': 528,\n",
       " 'feel': 529,\n",
       " 'always': 530,\n",
       " 'card': 531,\n",
       " 'st': 532,\n",
       " 'un': 533,\n",
       " 'redeemed': 534,\n",
       " '£800': 535,\n",
       " 'lol': 536,\n",
       " 'better': 537,\n",
       " 'yr': 538,\n",
       " 'must': 539,\n",
       " 'biz': 540,\n",
       " 'book': 541,\n",
       " 'people': 542,\n",
       " 'guys': 543,\n",
       " 'bored': 544,\n",
       " 'leh': 545,\n",
       " 'dat': 546,\n",
       " 'soon': 547,\n",
       " 'costa': 548,\n",
       " 'sol': 549,\n",
       " 'sk38xh': 550,\n",
       " \"uk's\": 551,\n",
       " '£10': 552,\n",
       " 'summer': 553,\n",
       " 'credit': 554,\n",
       " 'bid': 555,\n",
       " 'terms': 556,\n",
       " 'log': 557,\n",
       " '0870': 558,\n",
       " 'unlimited': 559,\n",
       " 'shit': 560,\n",
       " 'freephone': 561,\n",
       " 'said': 562,\n",
       " 'went': 563,\n",
       " 'ans': 564,\n",
       " 'loyalty': 565,\n",
       " 'meeting': 566,\n",
       " 'wont': 567,\n",
       " 'would': 568,\n",
       " 'food': 569,\n",
       " 'thought': 570,\n",
       " '02': 571,\n",
       " '06': 572,\n",
       " 'luv': 573,\n",
       " 'ending': 574,\n",
       " 'doing': 575,\n",
       " 'thats': 576,\n",
       " 'age16': 577,\n",
       " 'calling': 578,\n",
       " 'song': 579,\n",
       " 'each': 580,\n",
       " '11mths': 581,\n",
       " 'again': 582,\n",
       " '20': 583,\n",
       " 'motorola': 584,\n",
       " 'gay': 585,\n",
       " '08712460324': 586,\n",
       " '87077': 587,\n",
       " 'big': 588,\n",
       " 'handset': 589,\n",
       " 'year': 590,\n",
       " 'getzed': 591,\n",
       " 'flights': 592,\n",
       " 'spree': 593,\n",
       " 'shop': 594,\n",
       " \"t's\": 595,\n",
       " 'things': 596,\n",
       " 'called': 597,\n",
       " 'sunshine': 598,\n",
       " 'ringtones': 599,\n",
       " 'eat': 600,\n",
       " 'while': 601,\n",
       " 'left': 602,\n",
       " 'also': 603,\n",
       " 'weekends': 604,\n",
       " 'tot': 605,\n",
       " 'awesome': 606,\n",
       " 'boy': 607,\n",
       " 'standard': 608,\n",
       " 'forget': 609,\n",
       " '10am': 610,\n",
       " 'txtauction': 611,\n",
       " '£900': 612,\n",
       " 'yesterday': 613,\n",
       " 'area': 614,\n",
       " 'abt': 615,\n",
       " '85023': 616,\n",
       " 'unsub': 617,\n",
       " '09050090044': 618,\n",
       " 'toclaim': 619,\n",
       " 'pobox334': 620,\n",
       " 'stockport': 621,\n",
       " 'cost£1': 622,\n",
       " 'max10mins': 623,\n",
       " 'cheap': 624,\n",
       " 'company': 625,\n",
       " 'o2': 626,\n",
       " \"you've\": 627,\n",
       " 'world': 628,\n",
       " 'visit': 629,\n",
       " 'information': 630,\n",
       " 'user': 631,\n",
       " 'onto': 632,\n",
       " 'fantastic': 633,\n",
       " '1327': 634,\n",
       " 'croydon': 635,\n",
       " 'cr9': 636,\n",
       " '5wb': 637,\n",
       " 'entitled': 638,\n",
       " 'remove': 639,\n",
       " 'stuff': 640,\n",
       " 'minute': 641,\n",
       " 'bus': 642,\n",
       " 'takes': 643,\n",
       " 'entered': 644,\n",
       " 'person': 645,\n",
       " 'might': 646,\n",
       " 'crazy': 647,\n",
       " 'ts': 648,\n",
       " 'arrive': 649,\n",
       " 'maximize': 650,\n",
       " 'digital': 651,\n",
       " '28': 652,\n",
       " 'red': 653,\n",
       " 'working': 654,\n",
       " 'sky': 655,\n",
       " 'ldew': 656,\n",
       " 'down': 657,\n",
       " 'match': 658,\n",
       " '08712300220': 659,\n",
       " '08718720201': 660,\n",
       " 'll': 661,\n",
       " 'princess': 662,\n",
       " 'these': 663,\n",
       " 'store': 664,\n",
       " 'nothing': 665,\n",
       " 'rply': 666,\n",
       " 'sat': 667,\n",
       " 'enter': 668,\n",
       " 'never': 669,\n",
       " 'ldn': 670,\n",
       " 'b4': 671,\n",
       " 'local': 672,\n",
       " 'another': 673,\n",
       " 'long': 674,\n",
       " 'early': 675,\n",
       " 'logo': 676,\n",
       " 'ring': 677,\n",
       " 'mail': 678,\n",
       " 'finally': 679,\n",
       " '150pm': 680,\n",
       " 'woke': 681,\n",
       " 'cup': 682,\n",
       " 'official': 683,\n",
       " 'aight': 684,\n",
       " 'tenerife': 685,\n",
       " 'questions': 686,\n",
       " 'p': 687,\n",
       " 'few': 688,\n",
       " 'job': 689,\n",
       " 'didnt': 690,\n",
       " 'valentines': 691,\n",
       " 'hair': 692,\n",
       " 'liao': 693,\n",
       " 'asked': 694,\n",
       " 'fine': 695,\n",
       " 'pobox84': 696,\n",
       " 'fuck': 697,\n",
       " 'click': 698,\n",
       " 'awaiting': 699,\n",
       " 'wow': 700,\n",
       " 'sea': 701,\n",
       " 'yourself': 702,\n",
       " 'once': 703,\n",
       " 'watching': 704,\n",
       " 'driving': 705,\n",
       " 'member': 706,\n",
       " 'trip': 707,\n",
       " 'dis': 708,\n",
       " 'room': 709,\n",
       " 'hard': 710,\n",
       " 'computer': 711,\n",
       " \"won't\": 712,\n",
       " 'mind': 713,\n",
       " '80488': 714,\n",
       " 'knw': 715,\n",
       " '12': 716,\n",
       " 'hours': 717,\n",
       " 'does': 718,\n",
       " 'locations': 719,\n",
       " 'ec2a': 720,\n",
       " 'urawinner': 721,\n",
       " 'surprise': 722,\n",
       " 'break': 723,\n",
       " 'okie': 724,\n",
       " 'fantasies': 725,\n",
       " '08707509020': 726,\n",
       " 'records': 727,\n",
       " '86021': 728,\n",
       " 'access': 729,\n",
       " 'yet': 730,\n",
       " 'wants': 731,\n",
       " 'correct': 732,\n",
       " 'press': 733,\n",
       " 'ends': 734,\n",
       " 'contacted': 735,\n",
       " 'cum': 736,\n",
       " 'look': 737,\n",
       " 'cancel': 738,\n",
       " 'test': 739,\n",
       " 'forgot': 740,\n",
       " 'tmr': 741,\n",
       " 'oso': 742,\n",
       " 'asap': 743,\n",
       " 'months': 744,\n",
       " \"how's\": 745,\n",
       " '3g': 746,\n",
       " 'videophones': 747,\n",
       " 'videochat': 748,\n",
       " 'java': 749,\n",
       " 'dload': 750,\n",
       " 'noline': 751,\n",
       " 'rentl': 752,\n",
       " 'reference': 753,\n",
       " 'cc': 754,\n",
       " 'words': 755,\n",
       " 'pound': 756,\n",
       " 'cd': 757,\n",
       " 'id': 758,\n",
       " 'voda': 759,\n",
       " 'quoting': 760,\n",
       " 'den': 761,\n",
       " 'decimal': 762,\n",
       " 'wife': 763,\n",
       " 'sir': 764,\n",
       " 'actually': 765,\n",
       " 'though': 766,\n",
       " 'around': 767,\n",
       " 'show': 768,\n",
       " 'frnd': 769,\n",
       " '62468': 770,\n",
       " '25p': 771,\n",
       " '3030': 772,\n",
       " 'till': 773,\n",
       " 'missed': 774,\n",
       " 'plz': 775,\n",
       " 'bluetooth': 776,\n",
       " 'made': 777,\n",
       " 'pobox36504w45wq': 778,\n",
       " 'lar': 779,\n",
       " 'matches': 780,\n",
       " 'polyphonic': 781,\n",
       " 'hl': 782,\n",
       " 'partner': 783,\n",
       " 'simple': 784,\n",
       " 'zed': 785,\n",
       " 'wake': 786,\n",
       " 'lost': 787,\n",
       " 'plus': 788,\n",
       " 'until': 789,\n",
       " 'fa': 790,\n",
       " 'tkts': 791,\n",
       " '87121': 792,\n",
       " 'no1': 793,\n",
       " 'yer': 794,\n",
       " '84199': 795,\n",
       " 'eng': 796,\n",
       " 'box39822': 797,\n",
       " 'w111wx': 798,\n",
       " 'notice': 799,\n",
       " '5000': 800,\n",
       " 'tcs': 801,\n",
       " 'cw25wx': 802,\n",
       " 'na': 803,\n",
       " '1x150p': 804,\n",
       " 'car': 805,\n",
       " 'dad': 806,\n",
       " 'gonna': 807,\n",
       " 'plans': 808,\n",
       " 'little': 809,\n",
       " 'q': 810,\n",
       " 'country': 811,\n",
       " 'ansr': 812,\n",
       " 'tyrone': 813,\n",
       " 'fone': 814,\n",
       " 'write': 815,\n",
       " 'porn': 816,\n",
       " \"i'd\": 817,\n",
       " 'else': 818,\n",
       " 'plan': 819,\n",
       " 'put': 820,\n",
       " 'jordan': 821,\n",
       " '24': 822,\n",
       " 'recd': 823,\n",
       " 'cust': 824,\n",
       " 'old': 825,\n",
       " 'activate': 826,\n",
       " 'head': 827,\n",
       " 'smile': 828,\n",
       " '7pm': 829,\n",
       " '2optout': 830,\n",
       " 'wondering': 831,\n",
       " 'across': 832,\n",
       " 'evening': 833,\n",
       " 'probably': 834,\n",
       " 'nope': 835,\n",
       " 'hows': 836,\n",
       " \"didn't\": 837,\n",
       " '60p': 838,\n",
       " 'selection': 839,\n",
       " 'picked': 840,\n",
       " 'ppm': 841,\n",
       " 'action': 842,\n",
       " 'euro2004': 843,\n",
       " 'bed': 844,\n",
       " 'bank': 845,\n",
       " 'laid': 846,\n",
       " 'largest': 847,\n",
       " 'replying': 848,\n",
       " 'subscriber': 849,\n",
       " 'party': 850,\n",
       " \"doesn't\": 851,\n",
       " 'same': 852,\n",
       " 'extra': 853,\n",
       " 'tho': 854,\n",
       " 'brand': 855,\n",
       " '7250i': 856,\n",
       " 'w1jhl': 857,\n",
       " '0808': 858,\n",
       " '145': 859,\n",
       " '4742': 860,\n",
       " '9am': 861,\n",
       " '11pm': 862,\n",
       " 'stay': 863,\n",
       " 'anything': 864,\n",
       " 'registered': 865,\n",
       " 'receipt': 866,\n",
       " '80062': 867,\n",
       " 'arcade': 868,\n",
       " '434': 869,\n",
       " 'link': 870,\n",
       " 'college': 871,\n",
       " 'xx': 872,\n",
       " 'super': 873,\n",
       " 'check': 874,\n",
       " 'bloomberg': 875,\n",
       " 'dun': 876,\n",
       " 'girls': 877,\n",
       " 'horny': 878,\n",
       " 'til': 879,\n",
       " 'nite': 880,\n",
       " 'sch': 881,\n",
       " '81151': 882,\n",
       " '4t': 883,\n",
       " 'alone': 884,\n",
       " '40gb': 885,\n",
       " 'pod': 886,\n",
       " \"week's\": 887,\n",
       " 'competition': 888,\n",
       " 'wan': 889,\n",
       " 'loan': 890,\n",
       " 'purpose': 891,\n",
       " 'tenants': 892,\n",
       " 'post': 893,\n",
       " 'afternoon': 894,\n",
       " 'birthday': 895,\n",
       " 'mp3': 896,\n",
       " '83355': 897,\n",
       " 'pc': 898,\n",
       " 'listen': 899,\n",
       " 'than': 900,\n",
       " 'sipix': 901,\n",
       " '09061221066': 902,\n",
       " 'fromm': 903,\n",
       " 'ones': 904,\n",
       " 'contract': 905,\n",
       " 'house': 906,\n",
       " 'pix': 907,\n",
       " '8552': 908,\n",
       " 'vodafone': 909,\n",
       " 'fantasy': 910,\n",
       " 'dream': 911,\n",
       " 'team': 912,\n",
       " 'chennai': 913,\n",
       " 'app': 914,\n",
       " 'friday': 915,\n",
       " 'tncs': 916,\n",
       " 'read': 917,\n",
       " 'announcement': 918,\n",
       " 'vip': 919,\n",
       " 'tickets': 920,\n",
       " 'finish': 921,\n",
       " 'sport': 922,\n",
       " 'hand': 923,\n",
       " 'king': 924,\n",
       " 'credits': 925,\n",
       " 'don': 926,\n",
       " 'happen': 927,\n",
       " '88039': 928,\n",
       " 'skilgme': 929,\n",
       " \"he's\": 930,\n",
       " 'tariffs': 931,\n",
       " 'office': 932,\n",
       " 'children': 933,\n",
       " 'lonely': 934,\n",
       " 'anyone': 935,\n",
       " 'inviting': 936,\n",
       " 'paris': 937,\n",
       " 'wana': 938,\n",
       " 'bahamas': 939,\n",
       " '83600': 940,\n",
       " 'sale': 941,\n",
       " 'subscription': 942,\n",
       " '£4': 943,\n",
       " 'calls£1': 944,\n",
       " 'vary': 945,\n",
       " 'dunno': 946,\n",
       " 'juz': 947,\n",
       " '4u': 948,\n",
       " 'w': 949,\n",
       " 'thk': 950,\n",
       " 'tel': 951,\n",
       " 'eerie': 952,\n",
       " 'title': 953,\n",
       " 'set': 954,\n",
       " 'town': 955,\n",
       " 'singles': 956,\n",
       " 'high': 957,\n",
       " 'luck': 958,\n",
       " \"let's\": 959,\n",
       " 'drive': 960,\n",
       " 'moby': 961,\n",
       " '87131': 962,\n",
       " 'jus': 963,\n",
       " 'txtin': 964,\n",
       " '4info': 965,\n",
       " '786': 966,\n",
       " 'unredeemed': 967,\n",
       " '05': 968,\n",
       " 'london': 969,\n",
       " 'busy': 970,\n",
       " '2nite': 971,\n",
       " 'email': 972,\n",
       " 'girlfrnd': 973,\n",
       " 'comuk': 974,\n",
       " 'jamster': 975,\n",
       " 'é': 976,\n",
       " 'knew': 977,\n",
       " 'ref': 978,\n",
       " '250': 979,\n",
       " 'ho': 980,\n",
       " 'santa': 981,\n",
       " 'spook': 982,\n",
       " 'five': 983,\n",
       " 'cinema': 984,\n",
       " '09061209465': 985,\n",
       " 'suprman': 986,\n",
       " 'matrix3': 987,\n",
       " 'starwars3': 988,\n",
       " '2005': 989,\n",
       " '36504': 990,\n",
       " 'optout': 991,\n",
       " 'issues': 992,\n",
       " '08715705022': 993,\n",
       " 'able': 994,\n",
       " 'voicemail': 995,\n",
       " 'ya': 996,\n",
       " 'needs': 997,\n",
       " 'liverpool': 998,\n",
       " 'played': 999,\n",
       " 'original': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4194"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) # Importance of setting a max size of vocabulary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequencing and padding on training and testing \n",
    "max_len = 50 # Max number of tokens, used with truncating and padding\n",
    "trunc_type = \"post\" # Truncates sequences of tokens that are longer than max_len, post=right side\n",
    "padding_type = \"post\" # Pads AFTER (with post) if sequence is shorter than max_len\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(train_msg)\n",
    "training_padded = pad_sequences (training_sequences, maxlen = max_len,\n",
    "                                 padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(test_msg)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_len,\n",
    "                               padding = padding_type, truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training tensor:  (1195, 50)\n",
      "Shape of testing tensor:  (299, 50)\n"
     ]
    }
   ],
   "source": [
    "# Shape of train tensor\n",
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of testing tensor: ', testing_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to convert to torch tensors the data that I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_padded))\n",
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(training_padded).float()\n",
    "X_test = torch.from_numpy(testing_padded).float()\n",
    "Y_train = torch.from_numpy(train_labels).float()\n",
    "Y_test = torch.from_numpy(test_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(Y_train))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len # Number of input neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done\n",
      "Training accuracy is equal to 0.4694560766220093\n",
      "Testing accuracy is equal to 0.5183946490287781\n",
      "Iteration 2 done\n",
      "Training accuracy is equal to 0.5046024918556213\n",
      "Testing accuracy is equal to 0.52173912525177\n",
      "Iteration 3 done\n",
      "Training accuracy is equal to 0.5087866187095642\n",
      "Testing accuracy is equal to 0.695652186870575\n",
      "Iteration 4 done\n",
      "Training accuracy is equal to 0.701255202293396\n",
      "Testing accuracy is equal to 0.692307710647583\n",
      "Iteration 5 done\n",
      "Training accuracy is equal to 0.671966552734375\n",
      "Testing accuracy is equal to 0.6053511500358582\n",
      "Iteration 6 done\n",
      "Training accuracy is equal to 0.5966527462005615\n",
      "Testing accuracy is equal to 0.6354514956474304\n",
      "Iteration 7 done\n",
      "Training accuracy is equal to 0.6326360106468201\n",
      "Testing accuracy is equal to 0.7324414849281311\n",
      "Iteration 8 done\n",
      "Training accuracy is equal to 0.7146443724632263\n",
      "Testing accuracy is equal to 0.782608687877655\n",
      "Iteration 9 done\n",
      "Training accuracy is equal to 0.760669469833374\n",
      "Testing accuracy is equal to 0.7591972947120667\n",
      "Iteration 10 done\n",
      "Training accuracy is equal to 0.7623431086540222\n",
      "Testing accuracy is equal to 0.7591972947120667\n",
      "Iteration 11 done\n",
      "Training accuracy is equal to 0.7648535370826721\n",
      "Testing accuracy is equal to 0.7558528184890747\n",
      "Iteration 12 done\n",
      "Training accuracy is equal to 0.7456067204475403\n",
      "Testing accuracy is equal to 0.7458193898200989\n",
      "Iteration 13 done\n",
      "Training accuracy is equal to 0.7372384667396545\n",
      "Testing accuracy is equal to 0.7458193898200989\n",
      "Iteration 14 done\n",
      "Training accuracy is equal to 0.7405857443809509\n",
      "Testing accuracy is equal to 0.739130437374115\n",
      "Iteration 15 done\n",
      "Training accuracy is equal to 0.7531380653381348\n",
      "Testing accuracy is equal to 0.7558528184890747\n",
      "Iteration 16 done\n",
      "Training accuracy is equal to 0.7698744535446167\n",
      "Testing accuracy is equal to 0.7625418305397034\n",
      "Iteration 17 done\n",
      "Training accuracy is equal to 0.777405858039856\n",
      "Testing accuracy is equal to 0.7725752592086792\n",
      "Iteration 18 done\n",
      "Training accuracy is equal to 0.7748953700065613\n",
      "Testing accuracy is equal to 0.735785961151123\n",
      "Iteration 19 done\n",
      "Training accuracy is equal to 0.7665271759033203\n",
      "Testing accuracy is equal to 0.7224080562591553\n",
      "Iteration 20 done\n",
      "Training accuracy is equal to 0.7673640251159668\n",
      "Testing accuracy is equal to 0.7257525324821472\n",
      "Iteration 21 done\n",
      "Training accuracy is equal to 0.7740585803985596\n",
      "Testing accuracy is equal to 0.7458193898200989\n",
      "Iteration 22 done\n",
      "Training accuracy is equal to 0.7891213297843933\n",
      "Testing accuracy is equal to 0.7625418305397034\n",
      "Iteration 23 done\n",
      "Training accuracy is equal to 0.8058577179908752\n",
      "Testing accuracy is equal to 0.7625418305397034\n",
      "Iteration 24 done\n",
      "Training accuracy is equal to 0.8167364001274109\n",
      "Testing accuracy is equal to 0.7658863067626953\n",
      "Iteration 25 done\n",
      "Training accuracy is equal to 0.8133891224861145\n",
      "Testing accuracy is equal to 0.7591972947120667\n",
      "Iteration 26 done\n",
      "Training accuracy is equal to 0.8100418448448181\n",
      "Testing accuracy is equal to 0.7625418305397034\n",
      "Iteration 27 done\n",
      "Training accuracy is equal to 0.8025104403495789\n",
      "Testing accuracy is equal to 0.7658863067626953\n",
      "Iteration 28 done\n",
      "Training accuracy is equal to 0.8008368015289307\n",
      "Testing accuracy is equal to 0.7692307829856873\n",
      "Iteration 29 done\n",
      "Training accuracy is equal to 0.804184079170227\n",
      "Testing accuracy is equal to 0.7692307829856873\n",
      "Iteration 30 done\n",
      "Training accuracy is equal to 0.8083682060241699\n",
      "Testing accuracy is equal to 0.7658863067626953\n"
     ]
    }
   ],
   "source": [
    "iters = 30\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(max_len, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=.01)\n",
    "\n",
    "loss_history = [] #Per epoch\n",
    "accuracy_history = []\n",
    "accuracy_test_history = []\n",
    "\n",
    "for i in range(iters):\n",
    "    proba_pred = net(X_train)  # forward pass\n",
    "    proba_pred = proba_pred.squeeze(-1)  # transform the 1-element vectors into scalars\n",
    "\n",
    "    optimizer.zero_grad() # reset the gradients to 0\n",
    "    loss = criterion(proba_pred, Y_train)\n",
    "    loss_history.append(loss.item()) # .item() to turn it into a python number\n",
    "    loss.backward()  # obtain the gradients with respect to the loss\n",
    "    optimizer.step()  # perform one step of gradient descent\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_pred = proba_pred > 0.5  # Binary label\n",
    "        accuracy = (Y_train == Y_pred).float().mean()\n",
    "        accuracy_history.append(accuracy.item())\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        proba_pred_test = net(X_test)\n",
    "        proba_pred_test = proba_pred_test.squeeze(-1)\n",
    "        Y_pred_test = proba_pred_test > 0.5\n",
    "        accuracy_test = (Y_test == Y_pred_test).float().mean()\n",
    "        accuracy_test_history.append(accuracy_test.item())\n",
    "    \n",
    "    print(\"Iteration {iter} done\".format(iter=i+1))\n",
    "    print(\"Training accuracy is equal to {trainAcc}\".format(trainAcc=accuracy_history[-1]))\n",
    "    print(\"Testing accuracy is equal to {testAcc}\".format(testAcc=accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems to work and we have an instantaneous result.\n",
    "\n",
    "Many things to try, such as :\n",
    "1. Word embedding : the one we use has no reason to be good, we just order words by how often they appear, but it doesn't help us locate the words\n",
    "2. How to handle unbalanced dataset : discarding so many observations is not a great idea, we could also adapt our criterion to evaluate model (instead of just error rate)\n",
    "3. Another architecture for network\n",
    "    1. Convolutional network (why does it work ?)\n",
    "    2. LSTM or any sequential network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding : test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Embedding # keras.layers.embeddings marche pas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding, arguments : vocabulary size,\n",
    "   number of dimensions m in the embedding space represented by the Embedding layer,\n",
    "   the length n of each padded sequence\n",
    " \n",
    "Flatten : flattens the 2D output into a 1D array suitable for input to a Dense layer\n",
    "\n",
    "Dense layer : classifies the values emitted from the flatten layer, changing the number of neurons in the dense layer tends to maximize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 32)            16000     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 221,057\n",
      "Trainable params: 221,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "max_len = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only have a few hundred training samples, the embedding layer might not have enough information to properly vectorize the corpus of text. In that case, we can use pretrained word embeddings, but a network trained to classify text usually performs better when word embeddings are learned from the training data because such embeddings are task-specific. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on va se servir de training_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "48/48 [==============================] - 1s 8ms/step - loss: 0.4712 - accuracy: 0.7887 - val_loss: 0.2944 - val_accuracy: 0.8703\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2347 - accuracy: 0.9132 - val_loss: 0.1671 - val_accuracy: 0.9289\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9707 - val_loss: 0.1213 - val_accuracy: 0.9540\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9864 - val_loss: 0.1081 - val_accuracy: 0.9582\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.9927 - val_loss: 0.1067 - val_accuracy: 0.9540\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(training_padded, train_labels, validation_split=0.2, epochs=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically represent the accuracy of learning and validation for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from seaborn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from seaborn) (1.21.5)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\remac\\anaconda3\\envs\\dl_projet\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to do it as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment faire àpd nn.Sequential ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(training_padded).float()\n",
    "X_test = torch.from_numpy(testing_padded).float()\n",
    "Y_train = torch.from_numpy(train_labels).float()\n",
    "Y_test = torch.from_numpy(test_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(500, 32)\n",
       "  (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  (2): Linear(in_features=50, out_features=128, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Embedding(vocab_size, 32), #input_length = max_len),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(max_len, 128),\n",
    "    nn.ReLU(),\n",
    "    #nn.Linear(128, 128),\n",
    "    #nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m accuracy_test_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[1;32m----> 9\u001b[0m     proba_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     proba_pred \u001b[38;5;241m=\u001b[39m proba_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# transform the 1-element vectors into scalars\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# reset the gradients to 0\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\functional.py:2183\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2177\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2178\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2179\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=.01)\n",
    "\n",
    "loss_history = [] #Per epoch\n",
    "accuracy_history = []\n",
    "accuracy_test_history = []\n",
    "\n",
    "for i in range(iters):\n",
    "    proba_pred = net(X_train)  # forward pass\n",
    "    proba_pred = proba_pred.squeeze(-1)  # transform the 1-element vectors into scalars\n",
    "\n",
    "    optimizer.zero_grad() # reset the gradients to 0\n",
    "    loss = criterion(proba_pred, Y_train)\n",
    "    loss_history.append(loss.item()) # .item() to turn it into a python number\n",
    "    loss.backward()  # obtain the gradients with respect to the loss\n",
    "    optimizer.step()  # perform one step of gradient descent\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_pred = proba_pred > 0.5  # Binary label\n",
    "        accuracy = (Y_train == Y_pred).float().mean()\n",
    "        accuracy_history.append(accuracy.item())\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        proba_pred_test = net(X_test)\n",
    "        proba_pred_test = proba_pred_test.squeeze(-1)\n",
    "        Y_pred_test = proba_pred_test > 0.5\n",
    "        accuracy_test = (Y_test == Y_pred_test).float().mean()\n",
    "        accuracy_test_history.append(accuracy_test.item())\n",
    "    \n",
    "    print(\"Iteration {iter} done\".format(iter=i+1))\n",
    "    print(\"Training accuracy is equal to {trainAcc}\".format(trainAcc=accuracy_history[-1]))\n",
    "    print(\"Testing accuracy is equal to {testAcc}\".format(testAcc=accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_2 (TextVe (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 50, 32)            16000     \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 221,057\n",
      "Trainable params: 221,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n",
    "model.add(TextVectorization(max_tokens=vocab_size, output_sequence_length=max_len))\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125024"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['Why pay more for expensive meds when you can order them online and save $$$?'])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.message[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51352453"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([messages.message[0]])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pourquoi l'entraînement ne marche pas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [134]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1193\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1188\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1189\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1190\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1191\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1192\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1193\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1195\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "hist = model.fit(msg_df.message, msg_df.label, validation_split=0.2, epochs=5, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation en pratique ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.autogradtorch.aut as autograd\n",
    "# torch.autograd.grad\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context_word):\n",
    "        emb = self.embeddings(context_word)\n",
    "        hidden = self.linear(emb)\n",
    "        out = F.log_softmax(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (embeddings): Embedding(500, 2)\n",
       "  (linear): Linear(in_features=2, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2Vec(2, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remac\\AppData\\Local\\Temp\\ipykernel_11688\\4075335543.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(hidden)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1, 1195])) that is different to the input size (torch.Size([1, 1195, 50, 500])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [136]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m net(context_tensor)\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\modules\\loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\DL_projet\\lib\\site-packages\\torch\\nn\\functional.py:3056\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3054\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3059\u001b[0m     )\n\u001b[0;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3062\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([1, 1195])) that is different to the input size (torch.Size([1, 1195, 50, 500])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "vocabulary_size = vocab_size\n",
    "net = Word2Vec(embedding_size = 2, vocab_size = vocabulary_size)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "context_tensor_list = []\n",
    "\n",
    "for i in range(len(training_padded)):\n",
    "    target_tensor = torch.autograd.Variable(torch.LongTensor([train_labels]))\n",
    "    context_tensor = torch.autograd.Variable(torch.LongTensor([training_padded]))\n",
    "    context_tensor_list.append((target_tensor, context_tensor))\n",
    "    \n",
    "while True:\n",
    "    losses = []\n",
    "    #for i in range(len(training_padded)):\n",
    "    for target_tensor, context_tensor in context_tensor_list:\n",
    "        net.zero_grad()\n",
    "        log_probs = net(context_tensor)\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(\"Loss: \", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
